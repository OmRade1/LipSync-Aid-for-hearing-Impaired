
---

# **LipSync: AI-Powered Visual Speech Recognition for the Hearing Impaired** ğŸ‘ï¸â€ğŸ—¨ï¸  

LipSync is a cutting-edge deep learning-based **Visual Speech Recognition (VSR)** system designed to convert silent lip movement videos into transcribed text. Built with **Conv3D + BiLSTM** architecture and trained on the **GRID corpus dataset**, LipSync enables seamless communication support for individuals with hearing impairmentsâ€”functioning purely on visual input, unaffected by noise or sound-sensitive environments.

## ğŸš€ **Key Features**  

âœ… **Deep Learning Architecture:** 3D CNNs + BiLSTMs for accurate lip-based speech recognition  
âœ… **End-to-End Character-Level Transcription:** Uses **CTC Loss** for alignment-free decoding  
âœ… **Trained on GRID Corpus:** Structured English sentence prediction from visual speech  
âœ… **Robust Evaluation Metrics:** WER, SER, F1-score, Confusion Matrix analysis  
âœ… **High Accuracy:** Achieved **90.36% Weighted F1-score**  
âœ… **Lightweight UI with Streamlit:** Real-time processing & demos  
âœ… **Optimized for GPU Training:** Supports **RTX 4060** and higher  

---

## ğŸ— **System Architecture**  

```plaintext
[Video Input] â†’ [Frame Extraction] â†’ [Mouth ROI Cropping]  
â†’ [Conv3D Layers] â†’ [MaxPooling3D]  
â†’ [BiLSTM Layers] â†’ [Dense + Softmax]  
â†’ [CTC Loss Training] â†’ [Predicted Text Output]
```

---

## ğŸ“‚ **Dataset: GRID Corpus**  
- **~34,000** structured video samples  
- Includes **aligned video-transcript pairs**  
- Used for **model training & evaluation**  

---

## ğŸ”§ **Tech Stack**  

| **Domain** | **Tools / Frameworks** |
|------------|-----------------------|
| Deep Learning | TensorFlow 2.10, Keras |
| UI | Streamlit |
| Preprocessing | OpenCV, NumPy |
| Language | Python 3.8+ |
| Evaluation | WER, SER, F1 Score, Confusion Matrix |
| Deployment | Local GUI with `.h5` + `.pkl` models |

---

## ğŸš€ **Getting Started**  

### âœ… **Prerequisites**  
- **Python 3.8+**  
- **NVIDIA GPU (Recommended: 8GB+ VRAM)**  

### ğŸƒ **Installation & Execution**  

Clone the repository and install dependencies:  
```sh
pip install -r requirements.txt
```

Run the application via Streamlit:  
```sh
streamlit run streamlit_app.py
```

Upload a `.mpg` video from **GRID corpus**, click "Analyze Video," and view the transcribed output.

---

## ğŸ“ˆ **Model Performance**  

| **Metric** | **Value** |
|------------|---------|
| Accuracy | **90.38%** |
| F1 Score (Weighted) | **0.9036** |
| Word Error Rate (WER) | **~9.6%** |
| Sentence Error Rate (SER) | **~11%** |

Includes **confusion matrix**, accuracy plots, and training logs.

---

## ğŸ”® **Future Scope**  
ğŸ”¹ **Real-time webcam lip reading**  
ğŸ”¹ **Multilingual visual speech recognition**  
ğŸ”¹ **Advanced CTC decoding (e.g., Beam Search)**  
ğŸ”¹ **Audio-visual fusion models**  
ğŸ”¹ **Mobile & embedded deployment (TensorFlow Lite / ONNX)**  
ğŸ”¹ **Emotion-aware lip reading**  
ğŸ”¹ **Explainable AI (e.g., lip attention heatmaps)**  
ğŸ”¹ **Cloud-hosted interface enhancements**  

---

## ğŸ“š **References**  
- **GRID Corpus**  
- **TensorFlow 2.10**  
- **LipNet (Assael et al.)**  
- **LRS3 / LRW Datasets**  
- **BiLSTM, Conv3D, CTC Loss**  
- **Streamlit, OpenCV**  

---

ğŸ˜Š
